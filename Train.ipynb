{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.26/02\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import ROOT\n",
    "from ROOT import TMath\n",
    "from DataLoader import LoadDataAndProcess\n",
    "from Model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel: ttH has 576491 entries\n",
      "Channel: ggH has 1054711 entries\n",
      "Channel: WWH has 497468 entries\n",
      "Channel: data1 has 430344 entries\n",
      "Channel: data2 has 1528717 entries\n",
      "Channel: data3 has 2237187 entries\n",
      "Channel: data4 has 3602176 entries\n",
      "Processing Channel  ttH\n",
      "Processing Channel  ggH\n",
      "Processing Channel  WWH\n",
      "Processing Channel  data1\n",
      "Processing Channel  data2\n",
      "Processing Channel  data3\n",
      "Processing Channel  data4\n",
      "Processing Channel  ttH\n",
      "Processing Channel  ggH\n",
      "Processing Channel  WWH\n",
      "Processing Channel  data1\n",
      "Processing Channel  data2\n",
      "Processing Channel  data3\n",
      "Processing Channel  data4\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Importing the data from the ROOT files\n",
    "    All root data files can be obtained through the open data \n",
    "    program from the ATLAS collaboration. \n",
    "    They can be found here:\n",
    "        http://opendata.atlas.cern/samples-13tev/\n",
    "    All data files are in root format and require \n",
    "    the installation of ROOT's python interface. \n",
    "    ROOT is a CERN C++ Library which can be found here:\n",
    "        https://root.cern.ch/\n",
    "'''\n",
    "\n",
    "ttH = ROOT.TFile.Open(\"mc_341081.ttH125_gamgam.GamGam.root\")\n",
    "ggH = ROOT.TFile.Open(\"mc_343981.ggH125_gamgam.GamGam.root\")\n",
    "WWH = ROOT.TFile.Open(\"mc_345041.VBFH125_gamgam.GamGam.root\")\n",
    "data1 = ROOT.TFile.Open(\"data_A.GamGam.root\")\n",
    "data2 = ROOT.TFile.Open(\"data_B.GamGam.root\")\n",
    "data3 = ROOT.TFile.Open(\"data_C.GamGam.root\")\n",
    "data4 = ROOT.TFile.Open(\"data_D.GamGam.root\")\n",
    "\n",
    "# Channel Dictionary\n",
    "Channels = {\"ttH\":ttH.Get(\"mini\"),\n",
    "            \"ggH\":ggH.Get(\"mini\"),\n",
    "            \"WWH\":WWH.Get(\"mini\"), \n",
    "            'data1':data1.Get('mini'), \n",
    "            'data2':data2.Get('mini'),\n",
    "            'data3':data3.Get('mini'),\n",
    "            'data4':data4.Get('mini')\n",
    "        }\n",
    "\n",
    "# Loading the data into the dicrtionaries\n",
    "for channel in Channels:\n",
    "    print(\"Channel:\",channel,\"has\",Channels[channel].GetEntries(),\"entries\")\n",
    "\n",
    "# This outputs refer to the targets for the Neural Network\n",
    "# Initially all signal channels are differentiated from each other\n",
    "OutputMap = {'ttH':[1,0,0,0],'ggH':[0,1,0,0],'WWH':[0,0,1,0], \n",
    "             'data1':[0,0,0,1], 'data2':[0,0,0,1], 'data3':[0,0,0,1], 'data4':[0,0,0,1]}\n",
    "\n",
    "\n",
    "# Sort Photons by Energy\n",
    "def SortAndFlatten(Particles, SortFunction):\n",
    "    datapoint = []\n",
    "    Particles.sort(key = SortFunction)\n",
    "    for particle in Particles:\n",
    "        datapoint.append(particle.E())\n",
    "        datapoint.append(particle.Px())\n",
    "        datapoint.append(particle.Py())\n",
    "        datapoint.append(particle.Pz())\n",
    "    return datapoint\n",
    "\n",
    "# Compute invariant mass and sort by energy\n",
    "def SortFlattenAndInvariantMass(Particles, SortFunction):\n",
    "    datapoint = []\n",
    "    Particles.sort(key = SortFunction)\n",
    "    datapoint.append(np.sqrt(2*(Particles[0].E()*Particles[1].E()-Particles[0].Px()*Particles[1].Px()-Particles[0].Py()*Particles[1].Py()-Particles[0].Pz()*Particles[1].Pz())))\n",
    "    for particle in Particles:\n",
    "        datapoint.append(particle.Px())\n",
    "        datapoint.append(particle.Py())\n",
    "        datapoint.append(particle.Pz())\n",
    "    return datapoint\n",
    "\n",
    "'''\n",
    "    Loading both datasets from the ROOT files into \n",
    "    we also apply a sorting to both datasets\n",
    "\n",
    "'''\n",
    "\n",
    "UnprocessedLambda  = lambda Photons : SortAndFlatten( Photons, lambda Photon : -Photon.E() )\n",
    "UnprocessedDataset = LoadDataAndProcess(Channels, OutputMap, UnprocessedLambda, 50000 )\n",
    "\n",
    "PreprocessedLambda  = lambda Photons : SortFlattenAndInvariantMass(Photons, lambda Photon : -Photon.E() )\n",
    "PreprocessedDataset = LoadDataAndProcess(Channels, OutputMap, PreprocessedLambda, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnProcessedModel  = Model(\"Unprocessed Model\" ,\"./non-processed_weights2/\" ,UnprocessedDataset)\n",
    "PreprocessedModel = Model(\"Preprocessed Model\",\"./processed_weights2/\",PreprocessedDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnProcessedModel.train(0.35,100,True)\n",
    "PreprocessedModel.train(0.35,100,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UnProcessedModel.analyze()\n",
    "PreprocessedModel.analyze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "1cb35124d7b8b11792493ad9de20638983bfd92e094d4b9a16158140118f37cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
